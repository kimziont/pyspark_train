{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b1f180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7425dbc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/19 13:05:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db4d7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5dc9e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "531573c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_x = sc.parallelize(x)\n",
    "rdd_x_2 = sc.parallelize(x, 2)\n",
    "rdd_obama = sc.textFile(\"./obama.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c78149f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b528199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(rdd_x.getNumPartitions())\n",
    "print(rdd_x_2.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba59cb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect(): RDD의 모든 원소를 모아서 배열로 돌려준다\n",
    "# 액션 연산이다. \n",
    "# collect() 메서드를 호출한 서버의 메모리에 전체 데이터를 모두 담을 수 있을 정도의 충분한 메모리 공간이 확보돼 있어야 한다\n",
    "\n",
    "rdd_x.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23317854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_x.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ce57f7",
   "metadata": {},
   "source": [
    "## RDD 트랜스포메이션"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7a1306",
   "metadata": {},
   "source": [
    "- 맵: RDD 모든 요소에 함수를 적용해 새로운 RDD를 생성\n",
    "- 그룹화: 특정 조건에 따라 요소를 그룹화하거나 특정 함수를 적용\n",
    "- 집합: RDD간에 합집합, 교집합 등을 계산\n",
    "- 파티션: RDD의 파티션 개수를 조정\n",
    "- 필터: 특정 조건을 만족하는 요소만 선택\n",
    "- 정렬: 요소를 정해진 기준에 따라 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9252f664",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_x.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e78c80",
   "metadata": {},
   "source": [
    "### 맵 트랜스포메이션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "674e6efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map(): 메서드의 인자로 함수가 전달된다 (스파크는 스칼라 언어로 개발되었기 때문에, 함수형 프로그래밍을 지원한다)\n",
    "rdd_x_plus_1 = rdd_x.map(lambda x: x + 1)\n",
    "\n",
    "rdd_x_plus_1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2861180d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Barack',\n",
       " 'Hussein',\n",
       " 'Obama',\n",
       " 'II',\n",
       " 'is',\n",
       " 'an',\n",
       " 'American',\n",
       " 'politician',\n",
       " 'who',\n",
       " 'served']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatMap(): map()과의 차이는 인자로 전달되는 함수가 반환하는 값의 타입이 이터레이션이 가능한 컬렉션과 유사한 타입의 값을 반환해야 한다\n",
    "rdd_obama_words = rdd_obama.flatMap(lambda x: x.split(\" \"))\n",
    "rdd_obama_words.collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8fb13029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of map: [['a', 'b', 'c'], ['d', 'e', 'f']]\n",
      "result of flatmap: ['a', 'b', 'c', 'd', 'e', 'f']\n"
     ]
    }
   ],
   "source": [
    "# map() vs flatMap()\n",
    "# One of the use cases of flatMap() is to flatten column which contains arrays, list, or any nested collection\n",
    "x = ['a,b,c', 'd,e,f']\n",
    "rdd_x_map = sc.parallelize(x).map(lambda x: x.split(\",\"))\n",
    "rdd_x_flatmap = sc.parallelize(x).flatMap(lambda x: x.split(\",\"))\n",
    "print(f\"result of map: {rdd_x_map.collect()}\")\n",
    "print(f\"result of flatmap: {rdd_x_flatmap.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b67f383d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DB 연결\n",
      "DB 연결\n",
      "DB 연결\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 4, 7]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapPartitions()\n",
    "# 파티션 단위로 함수에 적용\n",
    "# 파티션에 속한 모든 요소를 한 번의 함수 호출로 처리\n",
    "# 파티션 단위의 중간 산출물을 만들거나, 데이터베이스 연결과 같은 고비용의 자원을 파티션 단위로 공유해 사용할 수 있다는 장점\n",
    "rdd_x = sc.parallelize(range(1, 11), 3)\n",
    "\n",
    "def increase(numbers):\n",
    "    print(\"DB 연결\")\n",
    "    return [numbers[0]]\n",
    "\n",
    "rdd_x_mapPartitions = rdd_x.mapPartitions(increase)\n",
    "\n",
    "rdd_x_mapPartitions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bdcc144e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 1), ('b', 1), ('c', 1), ('c', 1), ('c', 1), ('c', 1)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapValues(): 전달받은 함수를 페어RDD의 값(value)에 해당하는 요소에만 적용하고 그 결과를 반환 (모든 요소가 키(key)를 가지고 있어야함)\n",
    "\n",
    "rdd_key = sc.parallelize([\"a\", \"b\", \"b\", \"c\", \"c\", \"c\", \"c\"])\n",
    "\n",
    "pair_rdd = rdd_key.map(lambda x: (x, 0))\n",
    "\n",
    "pair_rdd_map_value = pair_rdd.mapValues(lambda x: x + 1)\n",
    "\n",
    "pair_rdd_map_value.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14f1bf7",
   "metadata": {},
   "source": [
    "### 그룹 트랜스포메이션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "80f9d1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, '가'), (2, '나'), (3, '다')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zip(): 첫 번째 컬렉션 RDD와 두 번째 컬렉션 RDD의 인덱스가 같은 원소끼리 그룹한 결과를 반환합니다\n",
    "\n",
    "rdd_num = sc.parallelize([1, 2, 3])\n",
    "rdd_kor = sc.parallelize([\"가\", \"나\", \"다\"])\n",
    "\n",
    "rdd_zip = rdd_num.zip(rdd_kor)\n",
    "rdd_zip.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8140e375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: even, value: [2, 4, 6, 8, 10]\n",
      "key: odd, value: [1, 3, 5, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "# groupBy(): RDD의 요소를 일정한 기준에 따라 여러 개의 그룹으로 나누고, 함수는 키를 반환하고, groupBy()는 그룹으로 구성된 새로운 RDD를 반환합니다\n",
    "rdd_nums = sc.parallelize(range(1, 11))\n",
    "\n",
    "rdd_groupBy = rdd_nums.groupBy(lambda x: \"even\" if x % 2 == 0 else \"odd\")\n",
    "\n",
    "for k, v in rdd_groupBy.collect():\n",
    "    print(f\"key: {k}, value: {list(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "254ac387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: b, value: [1, 1, 1]\n",
      "key: c, value: [1]\n",
      "key: a, value: [1, 1]\n"
     ]
    }
   ],
   "source": [
    "# groupByKey(): 페어RDD를 키(Key)로 그룹지어 결과를 반환한다\n",
    "\n",
    "pair_rdd = sc.parallelize([\"a\", \"a\", \"b\", \"b\", \"b\", \"c\"]).map(lambda x: (x, 1))\n",
    "\n",
    "for k, v in pair_rdd.groupByKey().collect():\n",
    "    print(f\"key: {k}, value: {list(v)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1263fd3c",
   "metadata": {},
   "source": [
    "### 집합 트랜스포메이션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3d9383bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 1, 2, 3]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distinct(): 중복을 제외한 요소로만 구성된 새로운 RDD를 반환한다\n",
    "rdd_raw = sc.parallelize([1, 3, 4, 2, 3, 3, 1, 1, 4])\n",
    "rdd_distinct = rdd_raw.distinct()\n",
    "rdd_distinct.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "19251d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, '가'), (1, '나'), (2, '가'), (2, '나'), (3, '가'), (3, '나')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cartesian(): 두 RDD 요소의 카르테시안 곱을 구하고 그 결과를 새로운 RDD로 반환\n",
    "rdd_num = sc.parallelize([1, 2, 3])\n",
    "rdd_kor = sc.parallelize(['가', '나'])\n",
    "\n",
    "rdd_cartesian = rdd_num.cartesian(rdd_kor)\n",
    "rdd_cartesian.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "16d518a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract()\n",
    "# union()\n",
    "# intersection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c7a59d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner Join: [('b', (1, 2)), ('c', (1, 2))]\n",
      "Left Outer Join: [('a', (1, None)), ('b', (1, 2)), ('c', (1, 2)), ('d', (1, None))]\n",
      "Right Outer Join: [('b', (1, 2)), ('c', (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "# join(): 두 RDD의 요소중 키(key)값이 같은 요소끼리 조인한 결과를 반환한다 (inner join)\n",
    "rdd1 = sc.parallelize([\"a\", \"b\", \"c\", \"d\"]).map(lambda x: (x, 1))\n",
    "rdd2 = sc.parallelize([\"b\", \"c\"]).map(lambda x: (x, 2))\n",
    "\n",
    "rdd_join = rdd1.join(rdd2)\n",
    "print(f\"Inner Join: {rdd_join.collect()}\")\n",
    "\n",
    "# leftOuterJoin()\n",
    "rdd_left_join = rdd1.leftOuterJoin(rdd2)\n",
    "print(f\"Left Outer Join: {rdd_left_join.collect()}\")\n",
    "\n",
    "# rightOuterJoin()\n",
    "rdd_right_join = rdd1.rightOuterJoin(rdd2)\n",
    "print(f\"Right Outer Join: {rdd_right_join.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dcba8d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subtractByKey(): subtract를 키(key)를 기준으로 한다\n",
    "\n",
    "rdd1 = sc.parallelize(['a', 'b']).map(lambda x: (x, 1))\n",
    "rdd2 = sc.parallelize(['b']).map(lambda x: (x, 1))\n",
    "\n",
    "rdd_sub_by_key = rdd1.subtractByKey(rdd2)\n",
    "rdd_sub_by_key.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a4aa1",
   "metadata": {},
   "source": [
    "### 집계 트랜스포메이션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d23d9650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 2), ('a', 1)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKey(): 같은 키(key)를 가진 값(value)들을 하나로 병합해 새로운 RDD를 반환한다\n",
    "# 병합을 수행하기 위해 두 개의 값을 하나로 합치는 함수를 인자로 전달해야 한다\n",
    "# 이 함수의 연산은 결합법칙과 교환법칙이 성립됨을 보장해야 한다\n",
    "# 왜냐하면 데이터가 여러 파티션에 분산되어 있어서 항상 같은 순서로 연산이 수행되지 않기 때문이다\n",
    "\n",
    "rdd = sc.parallelize(['a', 'b', 'b']).map(lambda x: (x, 1))\n",
    "rdd_reduce_by_key = rdd.reduceByKey(lambda x1, x2: x1 + x2)\n",
    "rdd_reduce_by_key.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ca8ef4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 2), ('c', 1), ('a', 1)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# foldByKey(): reduceByKey() 에 초기값을 전달할 수 있는 능력을 가지고 있다\n",
    "\n",
    "rdd = sc.parallelize(['a', 'b', 'b', 'c']).map(lambda x: (x, 1))\n",
    "rdd_fold_by_key = rdd.foldByKey(0, lambda x1, x2: x1 + x2)\n",
    "rdd_fold_by_key.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3be9b9",
   "metadata": {},
   "source": [
    "### 필터 트랜스포메이션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ca7045cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter()\n",
    "\n",
    "rdd = sc.parallelize(range(1, 11))\n",
    "\n",
    "rdd_filtered = rdd.filter(lambda x: x % 2 == 0)\n",
    "rdd_filtered.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6652e660",
   "metadata": {},
   "source": [
    "### 정렬 트랜스포메이션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ca76ec59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('d', 1), ('e', 1), ('z', 1)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortByKey()\n",
    "# 정렬후 파티션 내부의 요소는 정렬 순서상 인접한 요소로 재구성된다\n",
    "\n",
    "rdd = sc.parallelize(['a', 'e', 'z', 'd']).map(lambda x: (x, 1))\n",
    "\n",
    "rdd_sorted = rdd.sortByKey()\n",
    "rdd_sorted.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "07b2be1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['a', 'e', 'z', 'd']\n",
      "Values: [1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# keys(), values()\n",
    "\n",
    "rdd = sc.parallelize(['a', 'e', 'z', 'd']).map(lambda x: (x, 1))\n",
    "\n",
    "print(f\"Keys: {rdd.keys().collect()}\")\n",
    "print(f\"Values: {rdd.values().collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "509762cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 3, 4, 7, 8]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortBy()\n",
    "\n",
    "rdd = sc.parallelize([1, 4, 7, 2, 3, 8, 1])\n",
    "\n",
    "rdd_sorted = rdd.sortBy(lambda x: x)\n",
    "rdd_sorted.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b3bb1579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "복원 추출, 평균 발생 회수 0.5회: [1, 1, 1, 2, 10]\n",
      "복원 추출, 평균 발생 회수 5회: [1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10]\n",
      "비복원 추출, 샘플링 확률 0.1: []\n",
      "비복원 추출, 샘플링 확률 0.7: [1, 2, 3, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "# sample(withReplacement, fraction, seed)\n",
    "# withReplacement: true -> 복원 추출 -> 한 개 추출하고 다시 원래 상태로 돌리고 다시 추출\n",
    "# fraction: 복원 추출시 각 요소의 평균 발생 회수, 비복원 추출시 각 요소가 뽑힐 확률\n",
    "# seed: 반복 실행해도 결과를 일정하게 하기 위한 숫자\n",
    "\n",
    "# 주의할 점\n",
    "# 평균 발생 회수가 1회라고 N개의 결과가 N개가 됨을 보장하는 것이 아니고,\n",
    "# 샘플링 확률이 0.1이라고 해서 N개의 결과가 0.1N개가 됨을 보장하지 않는다\n",
    "# 전체 N개에 대해서 랜덤하게 0.3N개만 뽑고 싶은 경우에는 takeSample()을 사용한다 rdd.takeSample(False, 3) -> 결과는 리스트 -> 액션 연산\n",
    "\n",
    "# 균등분포가 아니라 확률분포 샘플링은 어떻게 할까?\n",
    "\n",
    "rdd = sc.parallelize(range(1, 11))\n",
    "\n",
    "rdd_sample1 = rdd.sample(True, 0.5)\n",
    "rdd_sample2 = rdd.sample(True, 5)\n",
    "\n",
    "rdd_sample3 = rdd.sample(False, 0.1)\n",
    "rdd_sample4 = rdd.sample(False, 0.7)\n",
    "\n",
    "print(f\"복원 추출, 평균 발생 회수 0.5회: {rdd_sample1.collect()}\")\n",
    "print(f\"복원 추출, 평균 발생 회수 5회: {rdd_sample2.collect()}\")\n",
    "print(f\"비복원 추출, 샘플링 확률 0.1: {rdd_sample3.collect()}\")\n",
    "print(f\"비복원 추출, 샘플링 확률 0.7: {rdd_sample4.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bdcc8b",
   "metadata": {},
   "source": [
    "## RDD 액션"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443de2a8",
   "metadata": {},
   "source": [
    "- 액션 연산이 호출되면 비로소 앞에 있던 모든 트랜스포메이션 연산이 실행된다\n",
    "- 문제는 이러한 Lazy Operation은 액션 연산이 호출될 때마다, 결과를 재사용하지 않고 계산을 한다는 것이다\n",
    "- 액션 연산 앞에 10개의 트랜스포메이션 연산이 있었다면 액션 연산을 3번 호출하면 총 30번의 트랜스포메이션 연산을 하게 된다\n",
    "- 결과를 반복 사용한다면 캐시를 적절히 이용해야 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "fddb8919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take()\n",
    "\n",
    "rdd = sc.parallelize(range(1, 11))\n",
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "969d74fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect()\n",
    "\n",
    "rdd = sc.parallelize(range(1, 11))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e4bf6fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count()\n",
    "\n",
    "rdd = sc.parallelize(range(1, 11))\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "13280d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 2, 2: 1, 3: 4})"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# countByValue()\n",
    "\n",
    "rdd = sc.parallelize([1, 1, 2, 3, 3, 3, 3])\n",
    "\n",
    "rdd.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b0e47fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce(): RDD에 포함된 모든 요소를 하나의 값으로 병합하고 결과를 반환\n",
    "# 모든 요소가 순서되로 처리되는 것 아님 (각 서버에 흩어져 있는 파티션 단위로 나눠져서 처리된다)\n",
    "\n",
    "rdd = sc.parallelize(range(1, 11))\n",
    "rdd.reduce(lambda x1, x2: x1 + x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "eb13cda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fold(): reduce()에 초기값을 지정할 수 있다\n",
    "\n",
    "rdd = sc.parallelize(range(1, 11))\n",
    "rdd.fold(0, lambda x1, x2: x1 + x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8d6e51b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum()\n",
    "\n",
    "rdd.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "69e4c9e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# foreach(): 인자로 전달받은 함수를 각 요소에 적용한다. 함수의 실행이 드라이버 프로그램이 실행된 서버가 아닌 각 개별 노드에서 실행된다\n",
    "# 각 노드에서 실행되도 의미있는 작업: DB, 파일시스템과 같은 다른 외부시스템과 통신\n",
    "rdd = sc.parallelize(range(1, 11))\n",
    "\n",
    "rdd.foreach(lambda x: x + 1)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d5eea25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[564] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache(), persist(), unpersist()\n",
    "rdd = sc.parallelize(range(1, 11))\n",
    "rdd.cache\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "rdd.persist(StorageLevel.MEMORY_AND_DISK_DESER)\n",
    "\n",
    "rdd.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bdd2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
